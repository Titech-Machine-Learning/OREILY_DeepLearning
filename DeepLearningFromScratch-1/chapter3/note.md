# ニューラルネットワーク

ニューラルネットワークは、適切な重みパラメータをデータから自動で学習できるという重要な性質がある。

## 活性化関数

入力信号の総和を出力信号に変換する関数のことを一般に、活性化関数と呼ぶ。

$$

y = h(b + w_1 x_1 + w_2 x_2)

$$

$$

h(x) =  \left \{
\begin{array}{l}
0 & (x \le 0) \\
1 & (x > 0)
\end{array}
\right.
$$

活性化関数は入力信号の総和がどのように活性化するか（どのように発火するか）ということを決定する役割がある。


## 3.2.1 シグモイド関数

$$h(x) = \frac{1}{ 1 + \exp(-x) }$$

> ニューラルネットワークでは活性化関数にシグモイド関数を用いて信号の変換を行い、その変換された信号が次のニューロンに伝えられます。実は、前章で見たパーセプトロンとこれから見ていくニューラルネットワークの主な違いは、この活性化関数だけなのです。その他の点 - ニューロンが多層につながる構造や、信号の伝達方法 - は基本的に前章のパーセプトロンと同じです。それでは、活性化関数として利用されるシグモイド関数について、ステップ関数と比較しながら詳しく見ていくことにしましょう。

## 3.2.3 ステップ関数

![ステップ関数](img/step.png)

上記のように、ステップ関数は0を境にして出力が0から1（または1から0）へ切り替わります。   
このような形状からステップ関数は階段関数と呼ばれることもあります。


## 3.2.4 シグモイド関数

![シグモイド関数](img/sigmoid.png)

## 3.2.5 シグモイド関数とステップ関数の比較

シグモイド関数は滑らか曲線であり、入力に対して連続的に出力が変化します。一方、ステップ関数は0を境に急に出力を変えています。このシグモイド関数の滑らかさが、ニューラルネットワークの学習において重要な意味を持ちます。

ニューラルネットワークでは、活性化関数に非線形関数を用いる必要があります。なぜならば、線形関数を用いるとニューラルネットワークで層を深くすることの意味がなくなってしまうからです。
詳しくは、書籍の説明に譲りますが、層を重ねることの恩恵を得るためには活性化関数に非線形関数を使う必要があります。

## 3.2.7 ReLU関数

![ReLU関数](img/relu.png)